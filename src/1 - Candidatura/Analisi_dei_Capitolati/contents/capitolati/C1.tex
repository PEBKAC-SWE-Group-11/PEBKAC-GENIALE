\subsection{Capitolato C1 - Artificial QI}
    \subsubsection{Informazioni generali}
        \begin{itemize}
            \item \textbf{Titolo}: Artificial QI
            \item \textbf{Proponente}: Zucchetti S.p.A.
            \item \textbf{Committente}: Prof. Vardanega T., Prof. Cardin R.
        \end{itemize}
     \subsubsection{Obiettivo}
    L’oggetto dell’appalto riguarda la realizzazione di un sistema che permetta di testare le capacità di risposta di modelli di IA, in particolare quelli basati su LLM, l’obiettivo è fornire uno strumento che consenta di valutare in modo approfondito come i modelli si comportano in vari contesti e con differenti configurazioni, evidenziando eventuali errori o variazioni

     \subsubsection{Dominio Applicativo}
    Lo scopo del progetto è quindi di interfacciarsi tramite API con modelli esterni per eseguire test basati su input prestabiliti, valutare la correttezza e coerenza delle risposte fornite dai modelli rispetto alle risposte attese e presentarne i risultati.
\begin{itemize}
    \item  La gestione di domande/risposte attese, esse verranno archiviate in un database, quindi sarà necessaria un’interfaccia user-friendly.
    \item  L’interrogazione ai LLM il sistema dovrà essere in grado di interrogare più modelli, permettendone il confronto tra di loro.
    \item  La valutazione della coerenza delle risposte verrà eseguita attraverso algoritmi di confronto come LLM più avanzati per esempio.
    \item I risultati dovranno essere aggregati e presentati in forma sintetica evidenziando i casi più problematici.
\end{itemize}
    \subsubsection{Tecnologie}
    Il progetto utilizza:

\begin{itemize}
    \item JSON come strumenti di valutazione automatica e analisi dei risultati.
    \item Database NoSQL per il Data Management.
    \item RAG,Fine-Tuning e strumenti di benchmarking per la valutazione e il testing delle LLM.
    \item AWS,Google Cloud come elementi di infrastruttura e ambienti di esecuzione.
    \item LLM come OpenAI GPT o Framework di NLP.
    \item Dashboard interattive per la visualizzazione ed il reporting.
\end{itemize}
    \subsubsection{Punti di forza}
    \begin{itemize}
    \item L’obiettivo centrale del progetto è testare l’accuratezza e la coerenza delle risposte di un LLM rispetto a domande predefinite. In questo modo si puo andare a migliorare a seguito di questi test le prestazioni degli LLM.
    \item Il capitolato consente di diverse tecnologie e modelli AI, permettendo agli sviluppatori di scegliere modelli con caratteristiche diverse in base ai risultati dei test applicati.
\end{itemize}
    \subsubsection{Punti deboli}
    \begin{itemize}
    \item Nella sezione delle risposte manca di una definizione chiara di criteri oggettivi di correttezza e verosimiglianza delle risposte. L’indicazione di valutare basate su modelli o dati di riferimento potrebbe risultare soggettiva senza delle metriche specifiche per stabilire quando una risposta è accettabile o meno.
    \item Vengono citati test automatici ma non viene fornito nessun piano dettagliato per la loro implementazione.
    \item Non viene citato come gestire il PoC, non è specificato come gestire le sfide esplorative del progetto essendo una LLM un campo a noi nuovo.
    \item Non è chiaro comee si prevede di valutare la correttezza di una risposta, si parla di “indici sintetici” per valutare la bontà delle risposte ma non viene indicato come uesto indice dovrebbe essere calcolato o come gestire il caso in cui le risposte non siano considerate soddisfacenti.
\end{itemize}
    \subsubsection{Conclusioni}
    Questo capitolato non è stato preso in considerazione durante la fase di selezione dei capitolati dato che l’ambito a cui appartiene non ha interessato particolarmente i membri del gruppo, facendo sì che l’attenzione ricadesse su altri capitolati.

% à, è, ì, ò, ù,
% À, È, Ì, Ò, Ù

% á, é, í, ó, ú, ý
% Á, É, Í, Ó, Ú, Ý